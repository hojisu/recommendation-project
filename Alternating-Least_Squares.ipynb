{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://jessesw.com/Rec-System/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_action = pd.read_csv('../input/제6회 L.POINT Big Data Competition-분석용데이터-01.온라인 행동 정보.csv', parse_dates=['sess_dt'])\n",
    "df_transaction = pd.read_csv('../input/제6회 L.POINT Big Data Competition-분석용데이터-02.거래 정보.csv', parse_dates=['de_dt'])\n",
    "# df_client_demo = pd.read_csv('../input/제6회 L.POINT Big Data Competition-분석용데이터-03.고객 Demographic 정보.csv')\n",
    "df_product = pd.read_csv('../input/제6회 L.POINT Big Data Competition-분석용데이터-04.상품분류 정보.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df_product['pd_c'] datatype : int -> object\n",
    "df_product['pd_c'] = df_product['pd_c'].apply(lambda num: \"{:04n}\".format(num))\n",
    "\n",
    "# df_transaction and df_prodcct merge!\n",
    "df_transaction = pd.merge(df_transaction, df_product, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599961, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clnt_id</th>\n",
       "      <th>trans_id</th>\n",
       "      <th>trans_seq</th>\n",
       "      <th>biz_unit</th>\n",
       "      <th>pd_c</th>\n",
       "      <th>de_dt</th>\n",
       "      <th>de_tm</th>\n",
       "      <th>buy_am</th>\n",
       "      <th>buy_ct</th>\n",
       "      <th>clac_nm1</th>\n",
       "      <th>clac_nm2</th>\n",
       "      <th>clac_nm3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>599956</td>\n",
       "      <td>35311</td>\n",
       "      <td>2839</td>\n",
       "      <td>3</td>\n",
       "      <td>B03</td>\n",
       "      <td>0339</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>14:09</td>\n",
       "      <td>3500</td>\n",
       "      <td>1</td>\n",
       "      <td>Cosmetics / Beauty Care</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Facial Masks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599957</td>\n",
       "      <td>35311</td>\n",
       "      <td>2839</td>\n",
       "      <td>4</td>\n",
       "      <td>B03</td>\n",
       "      <td>0339</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>14:09</td>\n",
       "      <td>3600</td>\n",
       "      <td>2</td>\n",
       "      <td>Cosmetics / Beauty Care</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Facial Masks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599958</td>\n",
       "      <td>35311</td>\n",
       "      <td>2839</td>\n",
       "      <td>5</td>\n",
       "      <td>B03</td>\n",
       "      <td>0339</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>14:09</td>\n",
       "      <td>3500</td>\n",
       "      <td>1</td>\n",
       "      <td>Cosmetics / Beauty Care</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Facial Masks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599959</td>\n",
       "      <td>35311</td>\n",
       "      <td>2839</td>\n",
       "      <td>2</td>\n",
       "      <td>B03</td>\n",
       "      <td>0339</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>14:09</td>\n",
       "      <td>4500</td>\n",
       "      <td>1</td>\n",
       "      <td>Cosmetics / Beauty Care</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Facial Masks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599960</td>\n",
       "      <td>35311</td>\n",
       "      <td>2839</td>\n",
       "      <td>6</td>\n",
       "      <td>B03</td>\n",
       "      <td>0324</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>14:09</td>\n",
       "      <td>6800</td>\n",
       "      <td>1</td>\n",
       "      <td>Cosmetics / Beauty Care</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>Nail Stickers / Stones</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        clnt_id  trans_id  trans_seq biz_unit  pd_c      de_dt  de_tm  buy_am  \\\n",
       "599956    35311      2839          3      B03  0339 2019-09-27  14:09    3500   \n",
       "599957    35311      2839          4      B03  0339 2019-09-27  14:09    3600   \n",
       "599958    35311      2839          5      B03  0339 2019-09-27  14:09    3500   \n",
       "599959    35311      2839          2      B03  0339 2019-09-27  14:09    4500   \n",
       "599960    35311      2839          6      B03  0324 2019-09-27  14:09    6800   \n",
       "\n",
       "        buy_ct                 clac_nm1   clac_nm2                clac_nm3  \n",
       "599956       1  Cosmetics / Beauty Care  Skin Care            Facial Masks  \n",
       "599957       2  Cosmetics / Beauty Care  Skin Care            Facial Masks  \n",
       "599958       1  Cosmetics / Beauty Care  Skin Care            Facial Masks  \n",
       "599959       1  Cosmetics / Beauty Care  Skin Care            Facial Masks  \n",
       "599960       1  Cosmetics / Beauty Care     Makeup  Nail Stickers / Stones  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_transaction.shape)\n",
    "df_transaction.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_c</th>\n",
       "      <th>clac_nm1</th>\n",
       "      <th>clac_nm2</th>\n",
       "      <th>clac_nm3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001</td>\n",
       "      <td>Automotive Products</td>\n",
       "      <td>Automotive Replacement Repair / Maintanance Kits</td>\n",
       "      <td>Automobile Oil / Additives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0002</td>\n",
       "      <td>Automotive Products</td>\n",
       "      <td>Automotive Replacement Repair / Maintanance Kits</td>\n",
       "      <td>Car Lights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0003</td>\n",
       "      <td>Automotive Products</td>\n",
       "      <td>Automotive Replacement Repair / Maintanance Kits</td>\n",
       "      <td>Car Paint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0004</td>\n",
       "      <td>Automotive Products</td>\n",
       "      <td>Automotive Replacement Repair / Maintanance Kits</td>\n",
       "      <td>Filters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0005</td>\n",
       "      <td>Automotive Products</td>\n",
       "      <td>Automotive Replacement Repair / Maintanance Kits</td>\n",
       "      <td>Wiper Blades</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pd_c             clac_nm1  \\\n",
       "0  0001  Automotive Products   \n",
       "1  0002  Automotive Products   \n",
       "2  0003  Automotive Products   \n",
       "3  0004  Automotive Products   \n",
       "4  0005  Automotive Products   \n",
       "\n",
       "                                           clac_nm2  \\\n",
       "0  Automotive Replacement Repair / Maintanance Kits   \n",
       "1  Automotive Replacement Repair / Maintanance Kits   \n",
       "2  Automotive Replacement Repair / Maintanance Kits   \n",
       "3  Automotive Replacement Repair / Maintanance Kits   \n",
       "4  Automotive Replacement Repair / Maintanance Kits   \n",
       "\n",
       "                     clac_nm3  \n",
       "0  Automobile Oil / Additives  \n",
       "1                  Car Lights  \n",
       "2                   Car Paint  \n",
       "3                     Filters  \n",
       "4                Wiper Blades  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transaction[['clnt_id', 'pd_c', 'buy_ct']]\n",
    "grouped_df =df.groupby(['clnt_id', 'pd_c']).sum().reset_index()\n",
    "\n",
    "# Replace a sum of zero purchases with a one to indicate purchased\n",
    "grouped_df.buy_ct.loc[grouped_df.buy_ct == 0] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of representing an explicit rating, the purchase quanity can represent a \"confidence\" in terms of how strong the interaction was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clnt_id</th>\n",
       "      <th>pd_c</th>\n",
       "      <th>buy_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0092</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clnt_id  pd_c  buy_ct\n",
       "0        2  0092      26\n",
       "1        2  0151       1\n",
       "2        2  0186       1\n",
       "3        2  0189       1\n",
       "4        2  0351       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only get customers where purchase totals were positive\n",
    "grouped_purchased = grouped_df.query('buy_ct > 0')\n",
    "grouped_purchased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = list(np.sort(grouped_purchased.clnt_id.unique()))\n",
    "products = list(grouped_purchased.pd_c.unique())\n",
    "quantity = list(grouped_purchased.buy_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = grouped_purchased.clnt_id.astype('category').cat.codes\n",
    "cols = grouped_purchased.pd_c.astype('category').cat.codes\n",
    "purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11284x1668 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 332184 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.23510209910768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 98.3% of the interaction matrix is sparse.\n",
    "matrix_size = purchases_sparse.shape[0] * purchases_sparse.shape[1]\n",
    "# Number of items interacted with\n",
    "num_purchases = len(purchases_sparse.nonzero()[0])\n",
    "sparsity = 100 * (1 - (num_purchases/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Traning and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return our training set, a test set that has been binarized to 0/1 for purchased/not purchased, and a list of which users had at least one item masked. We will test the performance of the recommender system on these users only. I am masking 20% of the user/item interactions for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(ratings, pct_test=0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "    \n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "    training set for later comparison to the test set, which contains all of the original ratings. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "    that originally had interaction set back to zero.\n",
    "    \n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "    compares with the actual interactions.\n",
    "    \n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    # Make a copy of the original set to be the test set. \n",
    "    test_set = ratings.copy()\n",
    "    # Store the test set as a binary preference matrix\n",
    "    test_set[test_set != 0] = 1\n",
    "    # Make a copy of the original data we can alter as our training set.\n",
    "    training_set = ratings.copy()\n",
    "    # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_inds = training_set.nonzero() \n",
    "    # Zip these pairs together of user,item index into list\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))\n",
    "    # Set the random seed to zero for reproducibility\n",
    "    random.seed(0)\n",
    "    # Round the number of samples needed to the nearest integer\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs)))\n",
    "    # Sample a random number of user-item pairs without replacement\n",
    "    samples = random.sample(nonzero_pairs, num_samples)\n",
    "    # Get the user row indices\n",
    "    user_inds = [index[0] for index in samples]\n",
    "    # Get the item column indices\n",
    "    item_inds = [index[1] for index in samples]\n",
    "    # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set[user_inds, item_inds] = 0 \n",
    "    # Get rid of zeros in sparse array storage after update to save space\n",
    "    training_set.eliminate_zeros()\n",
    "    \n",
    "    # Output the unique list of user rows that were altered \n",
    "    return training_set, test_set, list(set(user_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_train, product_test, product_users_altered = make_train(purchases_sparse, pct_test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_weighted_ALS(training_set, lambda_val=0.1, alpha=40, iterations=10, rank_size=20, seed=0):\n",
    "    '''\n",
    "    Implicit weighted ALS taken from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit\n",
    "    feedback based collaborative filtering. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - Our matrix of ratings with shape m x n, where m is the number of users and n is the number of items.\n",
    "    Should be a sparse csr matrix to save space. \n",
    "    \n",
    "    lambda_val - Used for regularization during alternating least squares. Increasing this value may increase bias\n",
    "    but decrease variance. Default is 0.1. \n",
    "    \n",
    "    alpha - The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. \n",
    "    The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between\n",
    "    various ratings.\n",
    "    \n",
    "    iterations - The number of times to alternate between both user feature vector and item feature vector in\n",
    "    alternating least squares. More iterations will allow better convergence at the cost of increased computation. \n",
    "    The authors found 10 iterations was sufficient, but more may be required to converge. \n",
    "    \n",
    "    rank_size - The number of latent features in the user/item feature vectors. The paper recommends varying this \n",
    "    between 20-200. Increasing the number of features may overfit but could reduce bias. \n",
    "    \n",
    "    seed - Set the seed for reproducible results\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The feature vectors for users and items. The dot product of these feature vectors should give you the expected \n",
    "    \"rating\" at each point in your original matrix. \n",
    "    '''\n",
    "    \n",
    "    # first set up our confidence matrix\n",
    "    \n",
    "    # To allow the matrix to stay sparse, I will add one later when each row is taken and converted to dence\n",
    "    conf = (alpha*training_set) \n",
    "    num_user = conf.shape[0]\n",
    "    # Get the size of our original ratings matrix, m X n \n",
    "    num_item = conf.shape[1]\n",
    "    \n",
    "    # initialize our X/Y feature vectors randomly with a set seed\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    # Random numbers in a m x rank shape\n",
    "    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size)))\n",
    "    # Normally this would be rank x n but we can transpose at the end. Makes calculation more simple.\n",
    "    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size)))\n",
    "    \n",
    "    X_eye = sparse.eye(num_user)\n",
    "    Y_eye = sparse.eye(num_item)\n",
    "    # Our regularization term lambda * I\n",
    "    lambda_eye = lambda_val * sparse.eye(rank_size)\n",
    "    \n",
    "    # We can compute this before iteration starts.\n",
    "    \n",
    "    # Begin iterations\n",
    "    \n",
    "    # Iterate back and forth between solving X given fixed Y and vice verse\n",
    "    for iter_step in range(iterations): \n",
    "        # Compute yTy and xTx at begining of each iteration to save computing time\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "        \n",
    "        # Being iteration to solve for X based on fixed Y \n",
    "        for u in range(num_user):\n",
    "            # Grab user row from confidence matrix and convert to dence\n",
    "            conf_samp = conf[u, :].toarray()\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1  # Create binarized preference vector \n",
    "            CuI = sparse.diags(conf_samp, [0]) # Get Cu - I term, don't need to subtract 1 since we never added it \n",
    "            yTCuIY = Y.T.dot(CuI).dot(Y) # This is the yT(Cu-I)Y term\n",
    "            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) # This is the yTCuPu term, where we add the eye back in Cu - I + I = Cu\n",
    "            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu)\n",
    "            # Solve for Xu = ((yTy + yT(Cu-I)Y + lambda*I)^-1)yTCuPu, equation 4 from the paper\n",
    "            \n",
    "        # Begin iteration to solve for Y based on fixed X\n",
    "        for i in range(num_item):\n",
    "            conf_samp = conf[:, i].T.toarray() # Transpose to get it in row format and convert to dense\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1 # Create binarized preference vector\n",
    "            CiI = sparse.diags(conf_samp, [0]) # Get Ci-I term, don't need to subtract 1 since we never added it\n",
    "            xTCiIX = X.T.dot(CiI).dot(X) # This is the xT(Cu-I)X term\n",
    "            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) # this is the xTCiPi term\n",
    "            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)\n",
    "            # Solve for Yi = ((xTx + xT(Cu-I)X + lambda*I)^-1)xTCiPi, equtation 5 from paper\n",
    "            \n",
    "        # End iterations\n",
    "        # Transpose at the end to make up for not being transposed at the begining.\n",
    "        # Y needs to be rank x n. Keep these as separate matrices for scale reasons \n",
    "        return X, Y.T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs, item_vecs = implicit_weighted_ALS(product_train, lambda_val=0.1, alpha=15, iterations=1, rank_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.98259849e-04, -5.34310642e-05,  8.47862757e-05, -1.12471669e-05,\n",
       "       -1.66714358e-04, -6.56827929e-04, -1.69063832e-04, -1.14847292e-04,\n",
       "       -7.04740757e-05,  9.18042650e-06])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vecs[0, :].dot(item_vecs).toarray()[0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    - predictions: your prediction output\n",
    "    \n",
    "    - test: the actual target result you are comparing to\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model \n",
    "    \n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "    \n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "    \n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "    \n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "    # An empty list to store the AUC for each user that had an item removed from the trainig set\n",
    "    store_auc = [] \n",
    "    # To store popular AUC scores\n",
    "    popularity_auc = []\n",
    "    # Get sum of item iterations to find most popular \n",
    "    pop_items = np.array(test_set.sum(axis=0)).reshape(-1)\n",
    "    item_vecs = predictions[1]\n",
    "    # Iterate through each user that had an item altered \n",
    "    for user in altered_users:\n",
    "        training_row = training_set[user,:].toarray().reshape(-1)  # Get sum of item iteractions to find most popular\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        \n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0, zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user,:].toarray()[0, zero_inds].reshape(-1)\n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training\n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        popularity_auc.append(acu_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "        \n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))\n",
    "    # Return the mean AUC rounded to there decimal places for both test and popularity benchmark\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_auc = [] \n",
    "popularity_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_items = np.array(product_train.sum(axis=0)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11284x20 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 225680 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.csr_matrix(user_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x1668 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 33360 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.csr_matrix(item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [user_vecs, item_vecs.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1668x20 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 33360 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_vecs= predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in product_users_altered:\n",
    "    training_row = training_set[user,:].toarray().reshape(-1)\n",
    "    zero_inds = np.where(training_row == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_row = product_train[1,:].toarray().reshape(-1)\n",
    "training_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-55464c6abc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# AUC for our recommender system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcalc_mean_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_users_altered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-465db15c2e4c>\u001b[0m in \u001b[0;36mcalc_mean_auc\u001b[0;34m(training_set, altered_users, predictions, test_set)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0muser_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Get only the items that were originally zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Select all ratings from the MF prediction for this user that originally had no iteraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "# AUC for our recommender system\n",
    "calc_mean_auc(product_train, product_users_altered, [user_vecs, item_vecs.T], product_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vec = predictions[0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_row = product_train[1,:].toarray().reshape(-1)\n",
    "zero_inds = np.where(training_row == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = user_vec.dot(item_vecs).toarray()[0, zero_inds].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.06017758e-04,  9.00187343e-05, -2.72807420e-05, ...,\n",
       "        1.24232536e-01,  4.20607641e-03,  1.86830711e-01])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
